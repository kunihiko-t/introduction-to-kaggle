<!-- sectionTitle: 機械学習の基礎:分類 -->

## 分類

---

- メールのスパム判定
- オンライン取引が詐欺かどうか
- 腫瘍が悪性か良性かどうか

など

---


## バイナリ分類(２項分類)

結果がYesかNoかなど二種類になるもの

<br/>

$$
\large y \in \{0,1\}
$$

<br/>

0: Negative Class


1: Positive Class  
 
<br/>

例えば

<br/>

$$
h_\theta(x) = \theta^Tx
$$

<br/>

とすると

<br/>

$$
If~h_\theta(x) \geq 0.5,~predict~"y=1"
$$

<br/>

$$
If~h_\theta(x) < 0.5,~predict~"y=0"
$$

<br/>
という感じにすれば二値に分類できる

---

## ロジスティック回帰分析

$$
\large 0 \leq h_\theta(x) \leq 1
$$

<br/>

$$
\large h_\theta (x) = g(\theta ^Tx)
$$

<br/>

$$
\large g(z) = \frac{1}{1+e^{-z}} : Sigmoid~function (Logistic~function)
$$

---

Sigmoid関数の形(必ず0から1の間に値が収まる)


![Sigmoid](https://cl.ly/40fdddaa949e/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-08%25252020.59.04.png)

---

## 仮説関数の出力の扱い

xを入力として渡した時にy=1となる確率

例：
$$
  x =
  \left[
    \begin{array}{rrr}
      x_0 \\
      x_1 \\
    \end{array}
  \right]
  =
  \left[
    \begin{array}{rrr}
      1 \\
      腫瘍のサイズ \\
    \end{array}
  \right]
$$

<br/>

もし特徴ベクトルx1,x2から

<br/>

$$

h_\theta(x) = 0.7

$$

<br/>

を得たとするとy=1である確率は70%であると言える
式だとこういう感じで書く↓
<br/>

$$
h_\theta(x) = P(y=1|x;\theta)
$$

<br/>

確率なので1になる確率と0になる確率を足すと1になる

<br/>

$$
P(y=1|x;\theta) + P(y=0|x;\theta) = 1
$$


---

まとめると

<br/>

$$
\large h_\theta (x) = g(\theta ^Tx) = P(y=1|x;\theta)

$$

<br/>

$$
\large g(z) = \frac{1}{1+e^{-z}}
$$

<br/>

$$
Suppose ~ predict ~ "y=1" ~ if ~ h_\theta(x) \geq 0.5
$$

<br/>

$$
predict ~ "y=0" ~ if ~ h_\theta(x) < 0.5
$$

<br/>

シグモイド関数は0以上の入力で0.5以上となり、0以下で0.5未満となるので↓が成り立つ

<br/>

$$
g(z) \geq 0.5~when~z \ge 0
$$

<br/>
<br/>

$$
h_\theta(x) = g(\theta^Tx) \ge 0.5
$$

<br/>

$$
whenever~ \theta^Tx \ge 0
$$

<br/>

つまり

<br/>

$$

\theta^Tx \ge 0~である時は常にy=1

$$

<br/>

$$

\theta^Tx < 0~である時は常にy=0

$$

<br/>

![sigmoid](https://cl.ly/ac0593314097/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252015.27.45.png)

---

## Decision Boundary (決定境界)

![Decision Boundary](https://cl.ly/2b84d35f4a66/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252015.40.41.png)

決定境界はトレーニングセットの性質ではなく仮説とパラメータの性質

---

## ロジスティック回帰分析の目的関数

---

## データの確認

<br/>

$$
Training~set : ~~{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})}
$$

<br/>

$$
m ~ examples:~~ x \in
  \left[
    \begin{array}{rrr}
      x_0 \\
      x_1 \\
      ... \\
      x_n \\
    \end{array}
  \right]
$$


<br/>

$$
x_0 = 1, ~ y\in {0,1}
$$

<br/>

$$

仮説関数: ~~h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}

$$

<br/>

どうやってパラメータθの値を決定するか？

---

## 目的関数(Cost Function)

線形回帰で使っていた式:

<br/>

$$

\large J(\theta) =  \frac{1}{m}\sum_{i = 1}^{m} \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)} )^2


$$

<br/>

これを以下のように定義する

<br/>

$$

\large Cost(h_\theta(x), y) = \frac{1}{2}(h_\theta(x)-y)^2

$$

<br/>


目的関数を使い回せるのかと思いきや、シグモイド関数が非線形なので、Jθを求めようとすると左のように局所解が何個もある状態になってしまうらしい
<br/>

![convex](https://cl.ly/6d1da35b2458/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252016.10.58.png)

---

## ロジスティック回帰の目的関数(Cost Function)

$$
\large Cost(h_\theta(x),y) = 

\left\{
\begin{array}{ll}
-log(h_\theta(x)) & if~y=1 \\
-log(1-h_\theta(x)) & if~y=0 \\
\end{array}
\right.
$$

<br/>
<br/>

$$

もしy=1でh_\theta(x)=1ならばコストは0となる。しかしh_\theta(x)=0であった場合コストは\inftyとなる。

$$

<br/>
<br/>

![y=1](https://cl.ly/317f2d9a120d/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252016.29.27.png)

<br/>

![y=0](https://cl.ly/25b895e59c53/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252016.33.42.png)

---

## ロジスティック回帰の目的関数(Cost Function)

yが必ず0か1になるので、下記のように式を１つにできる

<br/>

$$

\large Cost(h_\theta(x), y) = -y~log(h_\theta(x))-(1-y)log(1-h_\theta(x))

$$

<br/>
片方が必ず0になる

---


## Gradient Decent

$$
\large J(\theta) = - \frac{1}{m}\left[ \sum_{i = 1}^{m} y^{(i)}log~h\theta(x^{(i)})+(1-y^{(i)})log
(1-h_\theta(x^{(i)}))\right]
$$

<br/>

$$

J(\theta)の\thetaを最小化したい

$$

<br/>


$$
\large
Repeat:~\theta_j := \theta_j - \alpha \sum_{i=0}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}

$$

<br/>
θjの更新は同時にやる

<br/>
<br/>
線形回帰の時と仮説関数が違うだけなのでほぼ同じ。
これでθを求めることができる

---

## マルチクラスの分類問題

- one vs all classificationと呼ばれる
- メールのフォルダ分け、複数種類のタグ付け
- ２値分類ではyが0,1しか取らなかったが、マルチクラスの分類問題では1,2,3などカテゴリの数だけyの取りうる値が存在する

---
それぞれのクラスについて陰性か陽性かを判断する分類器を作り

入力を与えた時に最も数値（確率）が高かったものを採用すれば良い

<br/>


![figure](https://cl.ly/006096a9164c/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252018.11.18.png)

