<!-- sectionTitle: 過学習 -->

## 過学習(Overfitting)

---

## 過学習(Overfitting)

- そのモデルがトレーニングデータに対しては機能するが、未知のデータに対して機能せず、汎化できていない状態(Overfitting, High Variance)
- 逆に学習が足りていない場合はUnderfitting, High biasなどと呼ばれる

![overfitting](https://cl.ly/98f7f1fac40b/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252018.22.15.png)
![overfitting](https://cl.ly/779c7317619d/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-19%25252018.32.02.png)

---

## 過学習の解決方法

- 特徴量を減らす
  - 家の広さ、築年数、コンビニまでの距離などなど色々特徴量を与えれば与えるほど正確になると思いがちだが、重要でない特徴量は使わないことも重要（pandasとかで相関調べたり自動で抽出したり）
- 正則化(Regularization)
  - 学習の際に複雑さが増すことに対するペナルティを設ける
  - L1正則化とL2正則化がある
  - 値を0から1にするために行う正規化とは違うよ
- 与えられたデータ全てを使って学習を行うのではなく、訓練データ、   テストデータに分割して学習を行う
  - Stratified K-fold cross validationとかがよく使われる
 
時間がないので詳細については割愛するがとりあえずGoogle Colabolatoryでロジスティック回帰分析してみよう
 