<!-- sectionTitle: ニューラルネットワーク -->

##　ニューラルネットワーク

---

## ニューラルネットワーク


- 脳を模倣するようなシステム
- 80年代から90年代前半に流行っていたけど計算量が大きすぎる問題で90年代後半下火に
  - 近年のコンピュータの進化でまともに活用できるようになり人気復活

---

## ニューラルネットワーク

![Neural Network](https://cl.ly/64cebdb6bc09/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.07.18.png)

- 一番左が入力層(Input Layer)
- 出力以外の中間層が隠れ層(Hidden Layer)
- 最後の出力は出力層（Output Layer）
とそれぞれ呼ぶ
- 入力特徴量の値はa[0]という形で表され、0以降は活性化を表す
  - 活性化とはSigmoid関数やSoftmax関数、Relu関数など用途によって色々
  - 右上に[1]とか付いてるのは第1層という意味で、左下の添字は1番目の特徴量という意味
この図は２層のニューラルネットワーク。入力層は第0層と呼ばれ層にカウントしない
- wはWeightでbはバイアスユニットと呼ばれる
  - ロジスティック回帰で言うところのパラメータθとかx0の入力1みたいなやつ
  - この図の場合隠れ層のa[1]のweightの次元（Dimension）は(4,3)となりbiasは(4,1)となる
  - 次元はニューラルネットワークを構築する時ずれてたりするとエラーで落ちたりするので実装時は要注意
  - 出力層は一つなので何らかの数値になる。

---

## ニューラルネットワークがどのようにして出力を導くか

![Neural Network](https://cl.ly/7357af24dda5/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.30.00.png)

これはあるユニットだけを見た場合の図。

Activation unitの関数にはSigmoid関数を使っている。

（σって書いてあるのはシグモイド関数のこと）

<br/>

ほぼロジスティック回帰でやったやつですね。

--- 

## ニューラルネットワークがどのようにして出力を導くか

![Neural Network](https://cl.ly/8585f6a799ef/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.39.53.png)


- 1層目で何が行われているか式にすると上図のようになる
- 左から右に進むのでForward Propagationと呼ばれる
- forループを使うのは非効率なので転置してベクトル化している
- これは入力サンプルが１つのパターンだけど、m個あるパターンもまとめてベクトル化できる
---

## 最終出力までの次元数の遷移

![Dimension](https://cl.ly/e867298c31a7/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.52.34.png)


こういう次元数の遷移をしているから最終出力として実数が得られる

---

## Activation Function色々

![Activation Functions](https://cl.ly/f59a992aef3f/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252015.39.39.png)

- 隠れ層のActivation functionとして線形関数を使うと単に入力を線形関数で出力するものになってしまい意味がない
- 隠れ層には非線形の関数(Relu, tanh, sigmoidなど)を使おう

---

## ニューラルネットワークのGradient Decent

![Gradient Decent](https://cl.ly/1e938a043e7a/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-22%25252022.12.57.png)

- weightとbiasを求めたいのでここでもGradient Decentする

- 勾配を求める方法はBack Propagationと呼ぶ（上図の赤い矢印のように右から左に進むから。日本語だと誤差逆伝播）
  - 微分の連鎖律とか使って計算する
  - このへん難しくてちゃんと説明できる気がしないので割愛

---

## Deep Neural Network

![DNN](https://cl.ly/87f60000b0a2/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-22%25252022.56.18.png)

- 今までやってたのは隠れ層が１つのShallow Neural Network
- 隠れ層が多いとDeep Neural Networkと言われる
  - やっとここからDeep Learning
  - Kerasを使って動かしてみよう
