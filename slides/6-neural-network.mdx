<!-- sectionTitle: ニューラルネットワーク -->

##　ニューラルネットワーク

---

## ニューラルネットワーク


- 脳を模倣するようなシステム
- 80年代から90年代前半に流行っていたけど計算量が大きすぎる問題で90年代後半下火に
  - 近年のコンピュータの進化でまともに活用できるようになり人気復活

---

## ニューラルネットワーク

![Neural Network](https://cl.ly/64cebdb6bc09/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.07.18.png)

- 一番左が入力層(Input Layer)
- 出力以外の中間層が隠れ層(Hidden Layer)
- 最後の出力は出力層（Output Layer）
とそれぞれ呼ぶ
- 入力特徴量の値はa[0]という形で表され、0以降は活性化を表す
  - 活性化とはSigmoid関数やSoftmax関数、Relu関数など用途によって色々
  - 右上に[1]とか付いてるのは第1層という意味で、左下の添字は1番目の特徴量という意味
この図は２層のニューラルネットワーク。入力層は第0層と呼ばれ層にカウントしない
- wはWeightでbはバイアスユニットと呼ばれる
  - ロジスティック回帰で言うところのパラメータθとかx0の入力1みたいなやつ
  - この図の場合隠れ層のa[1]のweightの次元（Dimension）は(4,3)となりbiasは(4,1)となる
  - 次元はニューラルネットワークを構築する時ずれてたりするとエラーで落ちたりするので実装時は要注意
  - 出力層は一つなので何らかの数値になる。

---

## ニューラルネットワークがどのようにして出力を導くか

![Neural Network](https://cl.ly/7357af24dda5/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.30.00.png)

これはあるユニットだけを見た場合の図。

Activation unitの関数にはSigmoid関数を使っている。

（σって書いてあるのはシグモイド関数のこと）

<br/>

ほぼロジスティック回帰でやったやつですね。

--- 

## ニューラルネットワークがどのようにして出力を導くか

![Neural Network](https://cl.ly/8585f6a799ef/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.39.53.png)


- 1層目で何が行われているか式にすると上図のようになる
- 左から右に進むのでForward Propagationと呼ばれる
- forループを使うのは非効率なので転置してベクトル化している
- これは入力サンプルが１つのパターンだけど、m個あるパターンもまとめてベクトル化できる
---

## 最終出力までの次元数の遷移

![Dimension](https://cl.ly/e867298c31a7/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252014.52.34.png)


こういう次元数の遷移をしているから最終出力として実数が得られる

---

## Activation Function色々

![Activation Functions](https://cl.ly/f59a992aef3f/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-21%25252015.39.39.png)

- 隠れ層のActivation functionとして線形関数を使うと単に入力を線形関数で出力するものになってしまい意味がない
- 隠れ層には非線形の関数(Relu, tanh, sigmoidなど)を使おう

---

## ニューラルネットワークのGradient Decent

![Gradient Decent](https://cl.ly/1e938a043e7a/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-22%25252022.12.57.png)

- weightとbiasを求めたいのでここでもGradient Decentする

- 勾配を求める方法はBack Propagationと呼ぶ（上図の赤い矢印のように右から左に進むから。日本語だと誤差逆伝播）
  - 微分の連鎖律とか使って計算する
  - このへん難しくてちゃんと説明できる気がしないので割愛

---

## Deep Neural Network

![DNN](https://cl.ly/87f60000b0a2/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-22%25252022.56.18.png)

- 今までやってたのは隠れ層が１つのShallow Neural Network
- 隠れ層が多いとDeep Neural Networkと言われる
  - やっとここからDeep Learning
  - Kerasを使って動かしてみよう

---

## Convolutional Neural Network

- 畳み込みニューラルネットワーク
- 画像などデータ量がものすごく多いものに対して畳み込みを行う
 - 主に畳み込み層、Pooling層で構成される。よく使われるPoolingはMax pooling
  - 層の中のノードのうちいくつかを無効にして過学習を防ぐDropoutもよく一緒に使われる

![conv](https://cl.ly/87bc89bb9794/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-30%25252023.47.10.png)

---

## 畳み込み

- 入力に対して畳み込みを行う
  - 畳み込みのVisualization https://www.youtube.com/watch?v=f0t-OCG79-U

- 6px x 6pxのグレースケール画像だとこんな感じ。RGBだと３次元になる
- これにフィルタ(カーネルとも言う)をかけていくと4x4になる
![conv](https://cl.ly/19517f49194f/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-30%25252023.35.14.png)

---

## Max pooling

- 2x2でMax poolingを行うとこうなる
- データ量が少なくなって計算が早くなる

![pooling](https://cl.ly/28f8d2956605/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-30%25252023.42.17.png)

---

## Recurrent Neural Network

- 時系列データや自然言語処理など、連続した値を処理したい時によく使われる
- １つの入力だけでなく、前の入力も予測に使う

![rnn](https://cl.ly/05cf19337b0c/%2525E3%252582%2525B9%2525E3%252582%2525AF%2525E3%252583%2525AA%2525E3%252583%2525BC%2525E3%252583%2525B3%2525E3%252582%2525B7%2525E3%252583%2525A7%2525E3%252583%252583%2525E3%252583%252588%2525202019-09-30%25252023.56.23.png)


